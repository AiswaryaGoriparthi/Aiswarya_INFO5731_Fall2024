{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AiswaryaGoriparthi/Aiswarya_INFO5731_Fall2024/blob/main/Goriparthi_Aiswarya_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''\n",
        "Research Question:\n",
        "How does the combination of AI-driven chatbots with traditional human-based support systems impact customer satisfaction and resolution times?\n",
        "\n",
        "Information Required & the amount of data needed for analysis:-\n",
        "\n",
        "AI-Driven Chatbots: Data from 10-20 businesses employing AI chatbots.\n",
        "Resolution Times: Information on at least 1000 interactions, comparing resolution times between AI chatbots and human agents.\n",
        "Customer Satisfaction Scores: Feedback from 500-1000 customers who have experienced both AI and human support.\n",
        "Comparison Metrics: Interaction counts and issue complexity data from at least 500 cases per support system type.\n",
        "\n",
        "Detailed Steps for Collecting and Saving Data:-\n",
        "Define Objectives:\n",
        "Establish clear goals for comparing response times and satisfaction between AI chatbots and human agents.\n",
        "Determine Data Sources:\n",
        "Businesses: Select organizations using AI chatbots and those with traditional human support.\n",
        "Data Types: Gather feedback, interaction logs, resolution times, and issue complexity.\n",
        "Design Data Collection Methods:\n",
        "AI Chatbot Functionality: Review technical documentation and company reports.\n",
        "Resolution Time: Extract from system logs and timestamps.\n",
        "Customer Satisfaction: Use surveys and feedback forms.\n",
        "Comparative Measures: Document issue complexity and interaction counts.\n",
        "Data Collection Tools:\n",
        "Automated: Use web scraping or APIs.\n",
        "Manual: Collect data from surveys and feedback forms.\n",
        "Data Integrity:\n",
        "Validate and ensure consistency of data.\n",
        "Data Storage and Management:\n",
        "Set up a database, perform regular backups, and ensure data security with encryption and access controls.\n",
        "\n",
        "Detailed Steps for Saving Data:-\n",
        "1.Data Storage Setup:\n",
        "Database Design:Create a database schema with tables for survey answers, interaction logs, and other pertinent information.\n",
        "Information Store:To enable effective querying and analysis while working with enormous datasets, employ a data warehouse.\n",
        "2.Data Entry\n",
        "Import Data: To import gathered data into a database or data warehouse, employ database management tools or scripts.\n",
        "Arrange Information: Make certain that information is grouped and kept in an orderly fashion to facilitate retrieval.\n",
        "3.Data Restore\n",
        "Continual Backups: To avoid data loss, put in place a regular backup schedule.\n",
        "Storage: Safely store backups on physical media or in the cloud.\n",
        "Version control: To aid in data recovery, maintain track of several backup versions.\n",
        "4.Data Preparation and Cleaning\n",
        "Remove Duplicates: To find and remove duplicate records, use data cleansing tools.\n",
        "Data Normalization: Standardized data formats, including timestamps and ratings, are uniform.\n",
        "Dealing with Values that are missing: Use strategies to fill in the gaps in data or, if required, choose to omit records that are not complete.\n",
        "5.Information Security\n",
        "Access Control: Use user permissions to limit access to sensitive information.\n",
        "Encryption: To safeguard data while it is being transferred or stored, use encryption techniques.\n",
        "6.Data documentation: Keep records that describe the sources of the data, how they are collected, how they are stored, and any steps involved in any data cleansing.\n",
        "For future reference, include metadata that describes the data fields, formats, and context of the collection.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "52030df0-4425-4ba3-92bd-f2c66e5fcb32"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nResearch Question: \\nHow does the combination of AI-driven chatbots with traditional human-based support systems impact customer satisfaction and resolution times?\\n\\nInformation Required & the amount of data needed for analysis:-\\n\\nAI-Driven Chatbots: Data from 10-20 businesses employing AI chatbots.\\nResolution Times: Information on at least 1000 interactions, comparing resolution times between AI chatbots and human agents.\\nCustomer Satisfaction Scores: Feedback from 500-1000 customers who have experienced both AI and human support.\\nComparison Metrics: Interaction counts and issue complexity data from at least 500 cases per support system type.\\n\\nDetailed Steps for Collecting and Saving Data:-\\nDefine Objectives:\\nEstablish clear goals for comparing response times and satisfaction between AI chatbots and human agents.\\nDetermine Data Sources:\\nBusinesses: Select organizations using AI chatbots and those with traditional human support.\\nData Types: Gather feedback, interaction logs, resolution times, and issue complexity.\\nDesign Data Collection Methods:\\nAI Chatbot Functionality: Review technical documentation and company reports.\\nResolution Time: Extract from system logs and timestamps.\\nCustomer Satisfaction: Use surveys and feedback forms.\\nComparative Measures: Document issue complexity and interaction counts.\\nData Collection Tools:\\nAutomated: Use web scraping or APIs.\\nManual: Collect data from surveys and feedback forms.\\nData Integrity:\\nValidate and ensure consistency of data.\\nData Storage and Management:\\nSet up a database, perform regular backups, and ensure data security with encryption and access controls.\\n\\nDetailed Steps for Saving Data:- \\n1.Data Storage Setup:\\nDatabase Design:Create a database schema with tables for survey answers, interaction logs, and other pertinent information.\\nInformation Store:To enable effective querying and analysis while working with enormous datasets, employ a data warehouse.\\n2.Data Entry\\nImport Data: To import gathered data into a database or data warehouse, employ database management tools or scripts.\\nArrange Information: Make certain that information is grouped and kept in an orderly fashion to facilitate retrieval.\\n3.Data Restore\\nContinual Backups: To avoid data loss, put in place a regular backup schedule.\\nStorage: Safely store backups on physical media or in the cloud.\\nVersion control: To aid in data recovery, maintain track of several backup versions.\\n4.Data Preparation and Cleaning\\nRemove Duplicates: To find and remove duplicate records, use data cleansing tools.\\nData Normalization: Standardized data formats, including timestamps and ratings, are uniform.\\nDealing with Values that are missing: Use strategies to fill in the gaps in data or, if required, choose to omit records that are not complete.\\n5.Information Security\\nAccess Control: Use user permissions to limit access to sensitive information.\\nEncryption: To safeguard data while it is being transferred or stored, use encryption techniques.\\n6.Data documentation: Keep records that describe the sources of the data, how they are collected, how they are stored, and any steps involved in any data cleansing.\\nFor future reference, include metadata that describes the data fields, formats, and context of the collection.\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate synthetic customer support data\n",
        "def generate_data(num_samples):\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "\n",
        "    # Define possible values for support type, satisfaction scores, and issue complexity\n",
        "    support_types = ['AI', 'Human']\n",
        "    satisfaction_scores = np.arange(1, 11)  # Scores from 1 to 10\n",
        "    issue_complexities = ['Easy', 'Moderate', 'Complex']\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        support_type = np.random.choice(support_types)\n",
        "        resolution_time = np.random.randint(1, 61)  # Random resolution time between 1  60 minutes\n",
        "        satisfaction_score = np.random.choice(satisfaction_scores)\n",
        "        issue_complexity = np.random.choice(issue_complexities)\n",
        "\n",
        "        # If the support is by AI and issue is complex, random chance of escalation\n",
        "        if support_type == 'AI' and issue_complexity == 'Complex':\n",
        "            escalation_to_human = np.random.choice([True, False])\n",
        "        else:\n",
        "            escalation_to_human = False\n",
        "\n",
        "        # Append the data for the interaction\n",
        "        data.append({\n",
        "            'interaction_id': np.random.randint(100000, 999999),  # Random 6-digit interaction ID\n",
        "            'support_type': support_type,\n",
        "            'resolution_time_minutes': resolution_time,\n",
        "            'customer_satisfaction': satisfaction_score,\n",
        "            'issue_complexity': issue_complexity,\n",
        "            'escalation_to_human': escalation_to_human\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Generate 1000 samples\n",
        "num_samples = 1000\n",
        "data_samples = generate_data(num_samples)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data_samples)\n",
        "\n",
        "# Save collected data to a CSV file\n",
        "df.to_csv('customer_support_data.csv', index=False)\n",
        "\n",
        "print(\"Data collection complete. Saved to 'customer_support_data.csv'.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1bee4d-ec6d-443d-b698-28d00402b1c5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection complete. Saved to 'customer_support_data.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b60a85-7a44-43b1-c075-17ca112393e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported 1000 articles to acm_publications_xyz.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def retrieve_acm_publications(query, start_year, end_year, max_articles=1000):\n",
        "    base_search_url = \"https://dl.acm.org/action/doSearch\"\n",
        "\n",
        "    # Prepare a list to hold the collected publication data\n",
        "    publication_list = []\n",
        "    page_number = 0  # For paginated results\n",
        "\n",
        "    while len(publication_list) < max_articles:\n",
        "        full_url = f\"{base_search_url}?AllField={query}&startYear={start_year}&endYear={end_year}&pageSize=50&startPage={page_number}\"\n",
        "\n",
        "        # Perform the HTTP request\n",
        "        response = requests.get(full_url)\n",
        "        page_content = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Parse the search results\n",
        "        search_results = page_content.find_all(\"div\", class_=\"issue-item__content\")\n",
        "        for item in search_results:\n",
        "            # Extract publication title\n",
        "            title_tag = item.find(\"h5\", class_=\"issue-item__title\")\n",
        "            title = title_tag.text.strip() if title_tag else \"Unknown Title\"\n",
        "\n",
        "            # Extract author(s)\n",
        "            authors_tag = item.find(\"span\", class_=\"issue-item__authors\")\n",
        "            authors = authors_tag.text.strip() if authors_tag else \"Unknown Authors\"\n",
        "\n",
        "            # Extract venue\n",
        "            venue_tag = item.find(\"span\", class_=\"issue-item__venue\")\n",
        "            venue = venue_tag.text.strip() if venue_tag else \"Unknown Venue\"\n",
        "\n",
        "            # Extract publication year\n",
        "            year_tag = item.find(\"span\", class_=\"issue-item__year\")\n",
        "            year = year_tag.text.strip() if year_tag else \"Unknown Year\"\n",
        "\n",
        "            # Extract abstract\n",
        "            abstract_tag = item.find(\"div\", class_=\"issue-item__abstract\")\n",
        "            abstract = abstract_tag.text.strip() if abstract_tag else \"No Abstract Available\"\n",
        "\n",
        "            publication_list.append({\n",
        "                \"Title\": title,\n",
        "                \"Authors\": authors,\n",
        "                \"Venue\": venue,\n",
        "                \"Year\": year,\n",
        "                \"Abstract\": abstract\n",
        "            })\n",
        "\n",
        "            # Stop if the required number of articles has been reached\n",
        "            if len(publication_list) >= max_articles:\n",
        "                break\n",
        "\n",
        "        # Move to the next page\n",
        "        page_number += 1\n",
        "\n",
        "        # Exit loop if no more results are available\n",
        "        if not search_results:\n",
        "            break\n",
        "\n",
        "    return publication_list\n",
        "\n",
        "# Example usage\n",
        "search_keyword = \"XYZ\"\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "desired_articles_count = 1000\n",
        "\n",
        "collected_articles = retrieve_acm_publications(search_keyword, start_year, end_year, desired_articles_count)\n",
        "\n",
        "# Convert the list of articles to a DataFrame\n",
        "articles_df = pd.DataFrame(collected_articles)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_file = \"acm_publications_xyz.csv\"\n",
        "articles_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Exported {len(articles_df)} articles to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wt4bbgc0vdO",
        "outputId": "00c6680e-8a6a-4f33-8ec5-6a5a0844e5e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383c1471-e23f-41eb-8e8c-52b53c932121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Title  Score       ID  \\\n",
            "0   The Coming Wave of Freshman Failure. High-scho...   1091  18j2byq   \n",
            "1   America’s biggest education experiment is happ...    127  1dsswie   \n",
            "2   If we don't fix our unsafe classrooms, America...    346  13awu4u   \n",
            "3           We teach multiplication poorly in America     26  18kb2sw   \n",
            "4      A Critical Examination of Education in America      0  1cbxm4l   \n",
            "5     What killed the quality of education in the US?    307  1f6mhqk   \n",
            "6                               I'm Proud of Tim Walz    646  1eya2ll   \n",
            "7   Women outnumber men in the education systems a...    181  1fc8xnj   \n",
            "8            Best private schools in Central America?      0  1exe984   \n",
            "9   After working a decade in one of the poorest d...    318   ewz28n   \n",
            "10  Why isn't \"Critical Thinking\" part of America'...    127   u5u1vf   \n",
            "11          The teacher shortage will kill education     492  1b39rbz   \n",
            "12                       Weirdest College in America?     16  19e1zz1   \n",
            "13  Poverty Is America's #1 Education Problem. Tea...    244   1qhqvh   \n",
            "14  'I Work 3 Jobs And Donate Blood Plasma to Pay ...    102   9hc07u   \n",
            "\n",
            "                                                  URL  Created_Time_Stamp  \\\n",
            "0   https://www.reddit.com/r/education/comments/18...        1.702653e+09   \n",
            "1   https://www.reddit.com/r/education/comments/1d...        1.719840e+09   \n",
            "2   https://www.reddit.com/r/education/comments/13...        1.683480e+09   \n",
            "3   https://www.reddit.com/r/education/comments/18...        1.702795e+09   \n",
            "4   https://www.reddit.com/r/education/comments/1c...        1.713964e+09   \n",
            "5   https://www.reddit.com/r/education/comments/1f...        1.725218e+09   \n",
            "6   https://www.reddit.com/r/education/comments/1e...        1.724300e+09   \n",
            "7   https://www.reddit.com/r/education/comments/1f...        1.725831e+09   \n",
            "8   https://www.reddit.com/r/education/comments/1e...        1.724207e+09   \n",
            "9   https://www.reddit.com/r/education/comments/ew...        1.580523e+09   \n",
            "10  https://www.reddit.com/r/education/comments/u5...        1.650223e+09   \n",
            "11  https://www.reddit.com/r/education/comments/1b...        1.709237e+09   \n",
            "12  https://www.reddit.com/r/education/comments/19...        1.706051e+09   \n",
            "13  http://www.alternet.org/education/poverty-amer...        1.384298e+09   \n",
            "14       http://time.com/longform/teaching-in-america        1.537415e+09   \n",
            "\n",
            "    Number_of _Comments  \n",
            "0                   490  \n",
            "1                   139  \n",
            "2                   136  \n",
            "3                    97  \n",
            "4                    33  \n",
            "5                   554  \n",
            "6                   235  \n",
            "7                   686  \n",
            "8                     5  \n",
            "9                    87  \n",
            "10                   69  \n",
            "11                  511  \n",
            "12                   19  \n",
            "13                  108  \n",
            "14                  111  \n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Setup Reddit API connection\n",
        "reddit_client = praw.Reddit(\n",
        "    client_id='64lCmfNaNWoB5J7VZ8Stew',\n",
        "    client_secret='0nZuJNQ3zUK0YpVJhSmlg412FlEI2w',\n",
        "    user_agent='Aishu'\n",
        ")\n",
        "\n",
        "# Function to fetch posts from a specific subreddit containing a keyword\n",
        "def fetch_reddit_posts(subreddit_name, search_term, max_results=100):\n",
        "    subreddit = reddit_client.subreddit(subreddit_name)\n",
        "    search_results = subreddit.search(search_term, limit=max_results)\n",
        "\n",
        "    post_list = []\n",
        "    for post in search_results:\n",
        "        post_list.append({\n",
        "            'Title': post.title,\n",
        "            'Score': post.score,\n",
        "            'ID': post.id,\n",
        "            'URL': post.url,\n",
        "            'Created_Time_Stamp': post.created_utc,\n",
        "            'Number_of _Comments': post.num_comments\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(post_list)\n",
        "\n",
        "# Retrieve posts\n",
        "reddit_posts_df = fetch_reddit_posts('Education', 'America', 15)\n",
        "print(reddit_posts_df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "reddit_posts_df.to_csv('reddit_posts_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n"
      ],
      "metadata": {
        "id": "_RD3K4fXBr8Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "I got an invaluable learning experience has come from working on web scraping and data collection.\n",
        "Important ideas included managing pagination, using BeautifulSoup for HTML parsing, and PRAW for handling APIs.\n",
        "More over my capacity to gather and handle data efficiently was improved by my grasp of web structures and APIs.\n",
        "Adapting to different website structures and rate constraints presented difficulties.\n",
        "I modified my methods and adopted courteous scraping techniques, such as requesting more time, to deal with issues.\n",
        "what I understood is that Monitoring authentication and request frequency was necessary in order to comply with API rate constraints.\n",
        "My line of work greatly benefits from these abilities, which enable me to compile and evaluate big datasets from internet sources.\n",
        "By facilitating data-driven insights and increasing research opportunities, this capability improves research and makes it a crucial\n",
        "instrument for well-informed decision-making and comprehensive analysis.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5c470c49-69c1-4378-8a76-80c4d1eb3b54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWrite your response here.\\nI got an invaluable learning experience has come from working on web scraping and data collection.\\nImportant ideas included managing pagination, using BeautifulSoup for HTML parsing, and PRAW for handling APIs. \\nMore over my capacity to gather and handle data efficiently was improved by my grasp of web structures and APIs.\\nAdapting to different website structures and rate constraints presented difficulties. \\nI modified my methods and adopted courteous scraping techniques, such as requesting more time, to deal with issues. \\nwhat I understood is that Monitoring authentication and request frequency was necessary in order to comply with API rate constraints.\\nMy line of work greatly benefits from these abilities, which enable me to compile and evaluate big datasets from internet sources. \\nBy facilitating data-driven insights and increasing research opportunities, this capability improves research and makes it a crucial \\ninstrument for well-informed decision-making and comprehensive analysis.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}